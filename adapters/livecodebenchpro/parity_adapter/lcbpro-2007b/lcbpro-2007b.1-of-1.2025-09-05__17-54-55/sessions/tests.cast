{"version": 2, "width": 160, "height": 40, "timestamp": 1757121599, "env": {"SHELL": "/bin/bash", "TERM": "tmux-256color"}}
[0.103411, "o", "\u001b[?2004hroot@93b53e918556:/app# "]
[0.810876, "i", "clear\r"]
[0.844696, "o", "clear\r\n"]
[0.889314, "o", "\u001b[?2004l\r\u001b[H\u001b[J\u001b[3J"]
[0.897026, "o", "\u001b[?2004hroot@93b53e918556:/app# "]
[1.625958, "i", "bash /tests/run-tests.sh; tmux wait -S done\r"]
[1.634554, "o", "bash /tests/run-tests.sh; tmux wait -S done\r\n\u001b[?2004l\r"]
[2.339357, "o", "\u001b[1m===================================================================== test session starts ======================================================================\u001b[0m\r\nplatform linux -- Python 3.12.3, pytest-8.4.2, pluggy-1.6.0 -- /opt/venv/bin/python\r\ncachedir: .pytest_cache\r\nrootdir: /tests\r\n\u001b[1mcollecting ... \u001b[0m"]
[2.475636, "o", "\u001b[1m\rcollected 1 item                                                                                                                                               \u001b[0m\r\n"]
[2.476022, "o", "\r\ntest_outputs.py::test_external_judge "]
[8.234917, "o", "\u001b[31mFAILED\u001b[0m\u001b[31m                                                                                                              [100%]\u001b[0m"]
[8.249906, "o", "\r\n"]
[8.252403, "o", "\r\n=========================================================================== FAILURES ===========================================================================\r\n\u001b[31m\u001b[1m_____________________________________________________________________ test_external_judge ______________________________________________________________________\u001b[0m\r\n\u001b[1m\u001b[31mtest_outputs.py\u001b[0m:96: in test_external_judge\r\n"]
[8.320766, "o", "    \u001b[0m\u001b[94massert\u001b[39;49;00m _is_passed(res), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation failed: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mjson.dumps(res,\u001b[90m \u001b[39;49;00mensure_ascii=\u001b[94mFalse\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\r\n"]
[8.321973, "o", "\u001b[1m\u001b[31mE   AssertionError: evaluation failed: {\"status\": \"done\", \"passed\": false}\u001b[0m\r\n"]
[8.322132, "o", "\u001b[1m\u001b[31mE   assert False\u001b[0m\r\n"]
[8.3223, "o", "\u001b[1m\u001b[31mE    +  where False = _is_passed({'passed': False, 'status': 'done'})\u001b[0m\r\n"]
[8.322468, "o", "--------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------\r\n"]
[8.325171, "o", "Polling for submission 440 (timeout: 120s)\r\n  WAIT Submission 440 status: queued - 119s remaining\r\n  WAIT Submission 440 status: error - 118s remaining\r\n  WAIT Submission 440 status: error - 117s remaining\r\n  WAIT Submission 440 status: error - 115s remaining\r\n  OK Submission 440 completed"]
[8.325444, "o", "\r\n"]
[8.327159, "o", "\u001b[36m\u001b[1m=================================================================== short test summary info ====================================================================\u001b[0m\r\n"]
[8.327372, "o", "\u001b[31mFAILED\u001b[0m test_outputs.py::\u001b[1mtest_external_judge\u001b[0m - AssertionError: evaluation failed: {\"status\": \"done\", \"passed\": false}\r\n"]
[8.327643, "o", "\u001b[31m====================================================================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 5.99s\u001b[0m\u001b[31m =======================================================================\u001b[0m\r\n"]
[8.755478, "o", "\u001b[?2004hroot@93b53e918556:/app# "]
[9.327383, "i", "\u0004"]
[9.352282, "o", "\u001b[?2004l\r\r\n"]
[9.3524, "o", "exit\r\n"]
