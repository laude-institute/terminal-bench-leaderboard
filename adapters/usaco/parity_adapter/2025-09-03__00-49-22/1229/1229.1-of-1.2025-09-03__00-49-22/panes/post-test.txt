root@907fee4a17dd:/app# bash /tests/run-tests.sh; tmux wait -S done
Hit:1 http://deb.debian.org/debian bookworm InRelease
Hit:2 http://deb.debian.org/debian bookworm-updates InRelease
Hit:3 http://deb.debian.org/debian-security bookworm-security InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
curl is already the newest version (7.88.1-10+deb12u12).
0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.
downloading uv 0.7.13 aarch64-unknown-linux-gnu
no checksums to verify
installing to /root/.local/bin
  uv
  uvx
everything's installed!

To add $HOME/.local/bin to your PATH, either restart your shell or run:

    source $HOME/.local/bin/env (sh, bash, zsh)
    source $HOME/.local/bin/env.fish (fish)
Initialized project `app`
Using CPython 3.13.1 interpreter at: /usr/local/bin/python3.13
Creating virtual environment at: .venv
Resolved 8 packages in 384ms
Prepared 6 packages in 2.14s
Installed 6 packages in 41ms
 + iniconfig==2.1.0
 + numpy==2.3.2
 + packaging==25.0
 + pluggy==1.6.0
 + pygments==2.19.2
 + pytest==8.4.1
===================================================================== test session starts ======================================================================
platform linux -- Python 3.13.1, pytest-8.4.1, pluggy-1.6.0
rootdir: /tests
collected 2 items

../tests/test_outputs.py .F                                                                                                                              [100%]

=========================================================================== FAILURES ===========================================================================
_______________________________________________________________________ test_single_task _______________________________________________________________________

workdir = PosixPath('/tests')

    def test_single_task(workdir):
        # eval
        with open(workdir / "data" / "constraint.json", 'r') as f:
            constraint = json.load(f)

        task_id = constraint["cp_id"]
        with open("/app/solution.py", "r") as f:
            completion = f.read()
        solution_dict = {task_id: [completion]}
        problem_dict = {task_id: constraint}
>       result_sets = evaluate_solution_sets(
            workdir,
            solution_dict,
            problem_dict,
            mode="fail_fast"
        )

/tests/test_outputs.py:28:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/tests/evaluate.py:26: in evaluate_solution_sets
    result_sets = judge.judge(workdir, solution_dict, problem_dict, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/tests/judges/usaco_batch_judge.py:52: in judge
    result_list.append(future.result())
                       ^^^^^^^^^^^^^^^
/usr/local/lib/python3.13/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
/usr/local/lib/python3.13/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/usr/local/lib/python3.13/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/tests/judges/usaco_batch_judge.py:86: in usaco_judge
    return usaco_python3_judge(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

workdir = PosixPath('/tests'), problem_id = '1229'
solution_code = "#!/usr/bin/env python3\nimport sys\n\ndef main():\n    data = sys.stdin.read().split()\n    if not data:\n        ret...id\n        else:\n
        hi = mid - 1\n    sys.stdout.write(str(lo))\n\nif __name__ == '__main__':\n    main()\n"
runtime_limit = 600, memory_limit = 1280, num_tests = 11, mode = 'fail_fast'

    def usaco_python3_judge(workdir: Union[Path, str],
                            problem_id: str,
                            solution_code: str,
                            runtime_limit: float,
                            memory_limit: int,
                            num_tests: int,
                            mode: str = 'fail_fast') -> Result:
        '''
        mode = "fail_fast": returns the first result that fails, or ResultType.ACCEPTED
            if none fail
        mode = "eval_all": evaluates on all test cases no matter of result and returns a
            meta-result with a summary status and result_type, and a results_list of
            per-test-case Results (summary result_type of the first incorrect result, or
            ResultType.ACCEPTED)
        '''
        # (all show up as runtime error right now)
        assert mode == 'fail_fast' or mode == 'eval_all', \
            'Only modes fail_fast and eval_all are supported'

        # saving solution code if save_solution
        timestamp = datetime.now().strftime("%m_%d_%Y_%H_%M_%S_%f")

        result = {}
        if mode == 'eval_all':
            result['result_list'] = []
        result['problem_id'] = problem_id
        result['num_tests'] = num_tests
        assert num_tests > 0, 'Problem cannot have 0 tests'

        # construct judge sandbox directories if they don't exist
        path = Path(f"{workdir}/predictions/")
        path.mkdir(parents=True, exist_ok=True)
        pred_path = os.path.join(path.resolve(), '{}_{}.pred')

        for i in range(1, num_tests+1):
            input_file, output_file = get_test_in_out_files(workdir, problem_id, i)
            # include timestamp to avoid concurrency issues
            pred_file = pred_path.format(i, timestamp)

            # eval
            result_i = check_correctness(
                solution_code,
                i,
                runtime_limit,
                memory_limit,
                input_file,
                pred_file,
                output_file
            )
            if mode == 'fail_fast':
>               assert result_i['result_type'] == ResultType.ACCEPTED, \
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                    f"Failed because {result_i} on test {i}."
E               AssertionError: Failed because {'result_type': <ResultType.WRONG_ANSWER: 2>, 'status': 'wrong answer.\nInput:\n100\n1639 1493 8729 5453 37 8064
2087 615 1353 30 9290 8989 7846 7506 5660 5884 353 6167 9376 4808 6596 1045 5570 9073 7643 475 9146 6806 7742 6442 1658 7540 5171 5977 50 6157 8207 1161 6185 78
68 9315 194 3612 5026 9143 5707 8892 9240 7802 3465 6559 8632 7147 2885 9408 2473 2192 7032 9342 7106 7387 3457 5942 7911 5656 7136 5449 6124 5535 5240 3968 235
 7903 7342 2253 3204 6603 5068 7286 4501 6662 7890 6921 5542 9490 6621 5144 8844 2782 9333 5791 6215 3333 6547 3411 6859 6913 1062 9654 8493\n99\n2 1 1\n3 1 1\n
4 1 3\n5 1 1\n6 1 5\n7 1 3\n8 1 3\n9 1 4\n10 1 1\n11 1 5\n12 1 1\n13 1 11\n14 1 4\n15 1 10\n16 1 6\n17 1 6\n18 1 17\n19 1 12\n20 1 2\n21 1 16\n22 1 21\n23 1 6\n
24 1 7\n25 1 1\n26 1 2\n27 1 3\n28 1 3\n29 1 17\n30 1 4\n31 1 11\n32 1 11\n33 1 19\n34 1 31\n35 1 24\n36 1 4\n37 1 25\n38 1 22\n39 1 35\n40 1 33\n41 1 28\n42 1
34\n43 1 16\n44 1 7\n45 1 43\n46 1 8\n47 1 31\n48 1 39\n49 1 18\n50 1 20\n51 1 2\n52 1 17\n53 1 26\n54 1 26\n55 1 18\n56 1 10\n57 1 41\n58 1 13\n59 1 26\n60 1 3
4\n61 1 20\n62 1 30\n63 1 9\n64 1 35\n65 1 25\n66 1 24\n67 1 35\n68 1 47\n69 1 14\n70 1 45\n71 1 21\n72 1 41\n73 1 61\n74 1 24\n75 1 12\n76 1 13\n77 1 53\n78 1
70\n79 1 3\n80 1 67\n81 1 42\n82 1 48\n83 1 49\n84 1 78\n85 1 38\n86 1 71\n87 1 67\n88 1 19\n89 1 1\n90 1 78\n91 1 57\n92 1 32\n93 1 47\n94 1 42\n95 1 1\n96 1 1
1\n97 1 95\n98 1 66\n99 1 71\n100 1 65\nPredicted output:\n31924\nCorrect output:\n23431'} on test 4.

/tests/judges/usaco_batch_judge.py:146: AssertionError
======================================================================= warnings summary =======================================================================
test_outputs.py::test_single_task
test_outputs.py::test_single_task
test_outputs.py::test_single_task
test_outputs.py::test_single_task
test_outputs.py::test_single_task
test_outputs.py::test_single_task
test_outputs.py::test_single_task
test_outputs.py::test_single_task
  /usr/local/lib/python3.13/multiprocessing/popen_fork.py:67: DeprecationWarning: This process (pid=2971) is multi-threaded, use of fork() may lead to deadlocks
 in the child.
    self.pid = os.fork()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
============================================================================ PASSES ============================================================================
=================================================================== short test summary info ====================================================================
PASSED ../tests/test_outputs.py::test_solution_exists
FAILED ../tests/test_outputs.py::test_single_task - AssertionError: Failed because {'result_type': <ResultType.WRONG_ANSWER: 2>, 'status': 'wrong answer.\nInput
:\n100\n1639 1493 8729 5453 37 8064 2087 615 13...
=========================================================== 1 failed, 1 passed, 8 warnings in 0.43s ============================================================
root@907fee4a17dd:/app#
